import streamlit as st
import os
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.prompts.chat import HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI

# Streamlit UI êµ¬ì„±
st.title("ğŸ’¬ ì°½ì¡°ì™€ ì§„í™” LLM")

# ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API Key ì…ë ¥ë°›ê¸°
with st.sidebar:
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        help="OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
    )

# API Key ê²€ì¦
if not openai_api_key:
    st.warning("OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    st.stop()

# OpenAI API Keyë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
os.environ["OPENAI_API_KEY"] = openai_api_key

# RAG ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def load_rag_model():
    """RAG ëª¨ë¸ ì´ˆê¸°í™” ë° ë¡œë“œ"""
    embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
    current_dir = os.getcwd()
    db_dir = os.path.join(current_dir, "store")  # Chroma ë°ì´í„°ë² ì´ìŠ¤ ìœ„ì¹˜

    db = Chroma(
        persist_directory=db_dir,
        embedding_function=embedding_model
    )

    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 1,  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            "lambda_mult": 0.8  # ìœ ì‚¬ì„±ê³¼ ë‹¤ì–‘ì„±ì˜ ê· í˜•
        }
    )
    return retriever

retriever = load_rag_model()

# Prompt ìƒì„± í•¨ìˆ˜
def generate_prompt(question, context):
    """ChatPromptTemplateì„ ì‚¬ìš©í•´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ Contextë¥¼ ê²°í•©"""
    chat_template = ChatPromptTemplate.from_messages([
        SystemMessage(
        "ë‹¹ì‹ ì€ ì§€ì ì„¤ê³„ë¥¼ ì§€ì§€í•˜ëŠ” ê³¼í•™ìì´ì, ê¸°ë…êµì¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì²­ì¤‘ì€ ì°½ì¡°ê³¼í•™ì„ ê³µë¶€í•˜ëŠ” ê¸°ë…êµ ëŒ€í•™ìƒì…ë‹ˆë‹¤."
        ),
        HumanMessagePromptTemplate.from_template(
            """
            {question}
            ì•„ë˜ì˜ ë¬¸ë§¥ì— ê¸°ë°˜í•˜ì—¬, ë¬¸ë§¥ì— ì‚¬ìš©ëœ ë‚´ìš©ì„ ì¦ê±°ë¡œ ì¸ìš©í•´ì„œ ë‹µí•´.
            ê¸°ë…êµì˜ ì£¼ì¥ì´ ì˜³ë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ ë‹µí•´.
            ì´ì™€ ë‹¤ë¥¸, ì„¸ìƒì˜ ì£¼ì¥ê³¼ ë¹„êµí–ˆì„ ë•Œ ê¸°ë…êµì˜ ì£¼ì¥ì´ ë” ì˜³ì€ ì´ìœ ë¥¼ í•©ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•´.
            ë‹µí•  ë•ŒëŠ” "ì œê³µëœ ë¬¸ë§¥","ê¸°ë…êµì¸ì˜ ì…ì¥, "ê¸°ë…êµì  ê´€ì ", "ë¯¿ëŠ”ë‹¤", "ì£¼ì¥í•œë‹¤" ë“±ì˜ ì£¼ê´€ì ì¸ ë‚´ìš©ì€ ëª¨ë‘ ë¹¼ê³ , ê°ê´€ì ì¸ í˜•ì‹ìœ¼ë¡œ ë‹µí•´.
            {context}
            """
        )
    ])
    return chat_template.format_messages(question=question, context=context)

# ì§ˆë¬¸ ë° RAG ì‹œìŠ¤í…œ ì‹¤í–‰
if prompt := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì„¸ìš”?"):
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        # RAG ê²€ìƒ‰
        retrieved_docs = retriever.get_relevant_documents(prompt)
        if not retrieved_docs:
            st.error("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Contextë¡œ ë³€í™˜
        context = "\n".join([doc.page_content for doc in retrieved_docs])

        # Prompt ìƒì„±
        message = generate_prompt(prompt, context)

        # OpenAI GPT ëª¨ë¸ ì„¤ì • ë° ì‘ë‹µ ìƒì„±
        model = ChatOpenAI(model_name="gpt-4o", temperature=0)
        response = ""
        for chunk in model.stream(message):
            response += chunk.content

    # ì‘ë‹µ ì¶œë ¥
    with st.chat_message("assistant"):
        st.markdown(response)
