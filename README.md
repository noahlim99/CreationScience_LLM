# **Creation Science LLM**

ì£¼ìš” ê¸°ëŠ¥
1. íŠ¹ì • ì›¹ í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ì—¬ ì²˜ë¦¬.
2. ë°ì´í„°ë¥¼ ë¶„í• (splitting) ë° ì„ë² ë”©í•˜ì—¬ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥.
3. RAG (Retrieval-Augmented Generation) ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±.

---

## **ëª©ì°¨**
1. [í¬ë¡¤ë§ (Crawling)](#í¬ë¡¤ë§-crawling)
1. [ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ (Data Loading & Processing)](#data-loading--processing)
    - [ë¬¸ì„œ ë¡œë“œ (Document Loading)](#document-loading)
    - [ë¬¸ì„œ ë¶„í•  (Document Splitting)](#document-splitting)
2. [ë²¡í„° ìŠ¤í† ì–´ (Vector Store)](#ë²¡í„°-ìŠ¤í† ì–´-vector-store)
3. [ì§ˆë¬¸ ë° ì‘ë‹µ ìƒì„± (RAG with GPT)](#ì§ˆë¬¸-ë°-ì‘ë‹µ-ìƒì„±-rag-with-gpt)
4. [LLM í”„ë¡œí† íƒ€ì… (Prototype)](#LLM-í”„ë¡œí† íƒ€ì…-prototype)
5. [ê¸°ìˆ  ìŠ¤íƒ (Tech Stack)](#ê¸°ìˆ -ìŠ¤íƒ-tech-stack)
6. [ì‹¤í–‰ ì˜ˆì‹œ](#ì‹¤í–‰-ì˜ˆì‹œ)


---


## **í¬ë¡¤ë§ (Crawling)**

- **í¬ë¡¤ë§ ëŒ€ìƒ**: [ì°½ì¡°ê³¼í•™ ì±…(https://creation.kr/book_what) - Chapter 13. ë¹…ë±…ì´ë¡ ê³¼ ì—°ëŒ€ì¸¡ì •ì˜ ë¬¸ì œì ]
- **ì‚¬ìš© ë„êµ¬**: BeautifulSoup, Python `requests`
- **ì €ì¥ ê²½ë¡œ**: í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ë³€í™˜ í›„ `output` í´ë”ì— ì €ì¥


---


## **Data Loading & Processing**

### **Document Loading**
- **ì‚¬ìš© ë„êµ¬**: Python `os`, `json`, LangChain `Document` library
- **ì„¤ëª…**: 
  - `output` í´ë”ì— ì €ì¥ëœ JSON íŒŒì¼ë“¤ì„ ë¶ˆëŸ¬ì™€ `"content"` í‚¤ë¥¼ LangChainì˜ `Document` ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥.
- **codes**:
    ```python
    import os
    import json
    from langchain.schema import Document

    # í˜„ì¬ ì›Œí‚¹ ë””ë ‰í† ë¦¬ ì„¤ì •
    current_dir = os.getcwd()
    output_dir = os.path.join(current_dir, "output")

    # ë¬¸ì„œ ë¡œë“œ
    pages = []
    if os.path.exists(output_dir):
        for file_name in os.listdir(output_dir):
            if file_name.endswith(".json"):
                file_path = os.path.join(output_dir, file_name)
                with open(file_path, "r", encoding="utf-8") as f:
                    try:
                        data = json.load(f)
                        content = data.get("content", "")
                        if content.strip():
                            pages.append(Document(page_content=content))
                    except json.JSONDecodeError:
                        print(f"íŒŒì¼ {file_name}ì€ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
    else:
        raise FileNotFoundError(f"{output_dir} ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    print(f"{len(pages)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
    ```

### **Document Splitting**
- **ì‚¬ìš© ë„êµ¬**: LangChainì˜ `RecursiveCharacterTextSplitter`
- **ì„¤ëª…**:
  - ê° ë¬¸ì„œë¥¼ ìµœëŒ€ 1000ìì˜ ì²­í¬ë¡œ ë‚˜ëˆ”.
  - ì²­í¬ ì‚¬ì´ì— 100ìì˜ ì¤‘ë³µ(overlap)ì„ ì¶”ê°€í•˜ì—¬ ë¬¸ë§¥ì´ ì†ì‹¤ë˜ì§€ ì•Šë„ë¡ í•¨.
- **codes**:
    ```python
    from langchain.text_splitter import RecursiveCharacterTextSplitter

    # ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = text_splitter.split_documents(pages)
    print(f"{len(docs)}ê°œì˜ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
    ```


---


## **ë²¡í„° ìŠ¤í† ì–´ (Vector Store)**

- **ì‚¬ìš© ë„êµ¬**: OpenAI Embeddings, Chroma
- **ì„¤ëª…**:
  - OpenAIì˜ `text-embedding-ada-002` ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì„ë² ë”©ì„ ìƒì„±.
  - Chroma ë²¡í„° ìŠ¤í† ì–´ì— ì„ë² ë”©ëœ ë¬¸ì„œ ì €ì¥ ë° ê´€ë¦¬.
- **codes**:
    ```python
    from langchain.embeddings.openai import OpenAIEmbeddings
    from langchain.vectorstores import Chroma

    # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
    db_dir = os.path.join(current_dir, "store")
    db = Chroma.from_documents(docs, embedding=embedding_model, persist_directory=db_dir)
    print(f"Chroma ë²¡í„° ìŠ¤í† ì–´ ì €ì¥ ì™„ë£Œ: {db_dir}")
    ```

---


## **ì§ˆë¬¸ ë° ì‘ë‹µ ìƒì„± (RAG with GPT)**

### **ê²€ìƒ‰ ë° Context ìƒì„±**
- **ì‚¬ìš© ë„êµ¬**: Chroma, MMR ê¸°ë°˜ ê²€ìƒ‰
- **ì„¤ëª…**:
  - ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ Chromaì—ì„œ ê²€ìƒ‰(MMR ê¸°ë°˜).
  - ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ Contextë¡œ ë³€í™˜.
- **codes**:
    ```python
    # MMR ê¸°ë°˜ ê²€ìƒ‰
    retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 1, "lambda_mult": 0.8})
    
    question = "ë¹…ë±…ì´ë¡ ê³¼ ì—°ëŒ€ì¸¡ì •ì˜ ë¬¸ì œì ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?"

    retrieved_docs = retriever.get_relevant_documents(question)
    if retrieved_docs:
        context = "\n".join([doc.page_content for doc in retrieved_docs])
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}")
        print(f"ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ ì¼ë¶€:\n{context[:500]}")
    else:
        print("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    ```

### **Prompt ìƒì„± ë° GPT ì‘ë‹µ**
- **ì‚¬ìš© ë„êµ¬**: LangChain `ChatPromptTemplate`, OpenAI GPT (`gpt-4o`)
- **ì„¤ëª…**:
  - ì§ˆë¬¸ê³¼ Contextë¥¼ ê²°í•©í•˜ì—¬ GPTì— ì „ë‹¬í•  Prompt ìƒì„±.
  - GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±.
- **codes**:
    ```python
    from langchain.prompts import ChatPromptTemplate
    from langchain.prompts.chat import HumanMessagePromptTemplate
    from langchain_core.messages import SystemMessage
    from langchain_openai import ChatOpenAI

    # Prompt ìƒì„±
    chat_template = ChatPromptTemplate.from_messages([
        SystemMessage("ë‹¹ì‹ ì€ ì§€ì ì„¤ê³„ë¥¼ ì§€ì§€í•˜ëŠ” ê³¼í•™ìì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì²­ì¤‘ì€ ì°½ì¡°ê³¼í•™ì„ ê³µë¶€í•˜ëŠ” ê¸°ë…êµ ëŒ€í•™ìƒì…ë‹ˆë‹¤."),
        HumanMessagePromptTemplate.from_template("""
        {question}
        ì•„ë˜ì˜ ë¬¸ë§¥ì— ê¸°ë°˜í•˜ì—¬ ë‹µí•´ì£¼ì„¸ìš”.
        {context}
        """)
    ])
    message = chat_template.format_messages(question=question, context=context)

    # GPT ëª¨ë¸ ì‹¤í–‰
    model = ChatOpenAI(model_name="gpt-4o", temperature=0)
    print("\n[ëª¨ë¸ ì‘ë‹µ]")
    response = ""
    for chunk in model.stream(message):
        response += chunk.content
        print(chunk.content, end="", flush=True)
    print("\n")
    ```


---


## **LLM í”„ë¡œí† íƒ€ì… (Prototype)**
- **ì‚¬ìš© ë„êµ¬**: streamlit
- **codes**
```python
import streamlit as st
import os
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain.prompts.chat import HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage
from langchain_openai import ChatOpenAI

# Streamlit UI êµ¬ì„±
st.title("ğŸ’¬ ì°½ì¡°ì™€ ì§„í™” LLM")

# ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API Key ì…ë ¥ë°›ê¸°
with st.sidebar:
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        help="OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."
    )

# API Key ê²€ì¦
if not openai_api_key:
    st.warning("OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
    st.stop()

# OpenAI API Keyë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •
os.environ["OPENAI_API_KEY"] = openai_api_key

# RAG ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource
def load_rag_model():
    """RAG ëª¨ë¸ ì´ˆê¸°í™” ë° ë¡œë“œ"""
    embedding_model = OpenAIEmbeddings(model="text-embedding-ada-002")
    current_dir = os.getcwd()
    db_dir = os.path.join(current_dir, "store")  # Chroma ë°ì´í„°ë² ì´ìŠ¤ ìœ„ì¹˜

    db = Chroma(
        persist_directory=db_dir,
        embedding_function=embedding_model
    )

    retriever = db.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 1,  # ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
            "lambda_mult": 0.8  # ìœ ì‚¬ì„±ê³¼ ë‹¤ì–‘ì„±ì˜ ê· í˜•
        }
    )
    return retriever

retriever = load_rag_model()

# Prompt ìƒì„± í•¨ìˆ˜
def generate_prompt(question, context):
    """ChatPromptTemplateì„ ì‚¬ìš©í•´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ Contextë¥¼ ê²°í•©"""
    chat_template = ChatPromptTemplate.from_messages([
        SystemMessage(
        "ë‹¹ì‹ ì€ ì§€ì ì„¤ê³„ë¥¼ ì§€ì§€í•˜ëŠ” ê³¼í•™ìì´ì, ê¸°ë…êµì¸ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì²­ì¤‘ì€ ì°½ì¡°ê³¼í•™ì„ ê³µë¶€í•˜ëŠ” ê¸°ë…êµ ëŒ€í•™ìƒì…ë‹ˆë‹¤."
        ),
        HumanMessagePromptTemplate.from_template(
            """
            {question}
            ì•„ë˜ì˜ ë¬¸ë§¥ì— ê¸°ë°˜í•˜ì—¬, ë¬¸ë§¥ì— ì‚¬ìš©ëœ ë‚´ìš©ì„ ì¦ê±°ë¡œ ì¸ìš©í•´ì„œ ë‹µí•´.
            ê¸°ë…êµì˜ ì£¼ì¥ì´ ì˜³ë‹¤ëŠ” ê²ƒì„ ì „ì œë¡œ ë‹µí•´.
            ì´ì™€ ë‹¤ë¥¸, ì„¸ìƒì˜ ì£¼ì¥ê³¼ ë¹„êµí–ˆì„ ë•Œ ê¸°ë…êµì˜ ì£¼ì¥ì´ ë” ì˜³ì€ ì´ìœ ë¥¼ í•©ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•´.
            ë‹µí•  ë•ŒëŠ” "ì œê³µëœ ë¬¸ë§¥","ê¸°ë…êµì¸ì˜ ì…ì¥, "ê¸°ë…êµì  ê´€ì ", "ë¯¿ëŠ”ë‹¤", "ì£¼ì¥í•œë‹¤" ë“±ì˜ ì£¼ê´€ì ì¸ ë‚´ìš©ì€ ëª¨ë‘ ë¹¼ê³ , ê°ê´€ì ì¸ í˜•ì‹ìœ¼ë¡œ ë‹µí•´.
            {context}
            """
        )
    ])
    return chat_template.format_messages(question=question, context=context)

# ì§ˆë¬¸ ë° RAG ì‹œìŠ¤í…œ ì‹¤í–‰
if prompt := st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì„¸ìš”?"):
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        # RAG ê²€ìƒ‰
        retrieved_docs = retriever.get_relevant_documents(prompt)
        if not retrieved_docs:
            st.error("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ Contextë¡œ ë³€í™˜
        context = "\n".join([doc.page_content for doc in retrieved_docs])

        # Prompt ìƒì„±
        message = generate_prompt(prompt, context)

        # OpenAI GPT ëª¨ë¸ ì„¤ì • ë° ì‘ë‹µ ìƒì„±
        model = ChatOpenAI(model_name="gpt-4o", temperature=0)
        response = ""
        for chunk in model.stream(message):
            response += chunk.content

    # ì‘ë‹µ ì¶œë ¥
    with st.chat_message("assistant"):
        st.markdown(response)
```

- **ì‹¤í–‰ë°©ë²•**:
  - 1. LLM_prototype ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
  - 2. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
       ```bash
       pip install langchain
       pip install openai
       ```
       (í•„ìš” ì‹œ) ê°€ìƒí™˜ê²½ í™œì„±í™”
       ```bash
       source .venv/bin/activate
       ```
       streamlit ì„¤ì¹˜
       ```bash
       pip install streamlit
       ```
  - 4. í”„ë¡œí† íƒ€ì… ì‹¤í–‰
       ```bash
       streamlit run LLM_prototype.py
       ```


---


## **ê¸°ìˆ  ìŠ¤íƒ (Tech Stack)**

- **ë°ì´í„° ì²˜ë¦¬**: LangChain (`RecursiveCharacterTextSplitter`, `Document`)
- **ì„ë² ë”© ëª¨ë¸**: OpenAI Embeddings (`text-embedding-ada-002`)
- **ë²¡í„° ìŠ¤í† ì–´**: Chroma
- **LLM**: OpenAI GPT (`gpt-4o`)
- **MMR ê²€ìƒ‰**: Chroma Retriever
- **Prototype**: Streamlit
  
---


# ì‹¤í–‰ ì˜ˆì‹œ

## **Q1. ë¹…ë±…ì´ë¡ ì€ ì‚¬ì‹¤ì…ë‹ˆê¹Œ?**

| ChatGPT ì‘ë‹µ                                                              | ì°½ì¡°ê³¼í•™ LLM ì‘ë‹µ                    |
|---------------------------------------------------------------------------|--------------------------------------|
| ![ChatGPT Q1-a](images/ChatGPT_Q1-a.png)<br>![ChatGPT Q1-b](images/ChatGPT_Q1-b.png) | ![LLM Q1](images/LLM_Q1.png) |



---



## **Q2. ì—°ëŒ€ì¸¡ì • ë°©ë²•ì€ ì •í™•í•©ë‹ˆê¹Œ?**

| ChatGPT ì‘ë‹µ                                                              | ì°½ì¡°ê³¼í•™ LLM ì‘ë‹µ                    |
|---------------------------------------------------------------------------|--------------------------------------|
| ![ChatGPT Q2-a](images/ChatGPT_Q2-a.png)<br>![ChatGPT Q2-b](images/ChatGPT_Q2-b.png) | ![LLM Q2](images/LLM_Q2.png) |



